# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-26T05:37:52.023006Z","iopub.execute_input":"2026-02-26T05:37:52.023219Z","iopub.status.idle":"2026-02-26T05:38:21.715956Z","shell.execute_reply.started":"2026-02-26T05:37:52.023197Z","shell.execute_reply":"2026-02-26T05:38:21.715315Z"}}
"""
BERTå®Œå½¢å¡«ç©ºæµ‹è¯•å…¬å…±å‡½æ•°åº“
æä¾›æ¨¡å‹åŠ è½½ã€è¯„åˆ†ã€è‡ªå›å½’ç­”é¢˜ç­‰é€šç”¨åŠŸèƒ½
"""

import re
from typing import Dict, List, Optional, Tuple

import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer


def setup_device() -> torch.device:
    """
    è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    
    Returns:
        torch.device: å¯ç”¨çš„è®¡ç®—è®¾å¤‡
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"å½“å‰è®¡ç®—è®¾å¤‡: {device}")
    return device


def load_model(model_name: str = "roberta-base", device: Optional[torch.device] = None) -> Tuple:
    """
    åŠ è½½é¢„è®­ç»ƒçš„MLMæ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "roberta-base"
        device: è®¡ç®—è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
        
    Returns:
        tuple: (tokenizer, model, device)
    """
    if device is None:
        device = setup_device()
    
    print(f"æ­£åœ¨åŠ è½½ {model_name} åˆ°æ˜¾å­˜...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)
    model.eval()
    
    return tokenizer, model, device


def score_sentence(
    sentence: str,
    tokenizer,
    model,
    device: torch.device,
    max_length: int = 512
) -> float:
    """
    è®¡ç®—å¥å­çš„ä¸è‡ªç„¶åº¦ï¼ˆlosså€¼ï¼‰- æ—§æ–¹æ³•ï¼Œç”¨äºå‘åå…¼å®¹
    
    Args:
        sentence: å¾…è¯„åˆ†çš„å¥å­
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        max_length: æœ€å¤§tokené•¿åº¦
        
    Returns:
        float: losså€¼ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    """
    inputs = tokenizer(
        sentence,
        return_tensors="pt",
        truncation=True,
        max_length=max_length
    )
    inputs["labels"] = inputs["input_ids"].clone()
    
    # å°†æ•°æ®é€å…¥è®¾å¤‡
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        loss = outputs.loss.item()
    
    return loss


def score_candidate_word(
    text_with_mask: str,
    candidate_word: str,
    tokenizer,
    model,
    device: torch.device,
    max_length: int = 512,
    stride: int = 128
) -> float:
    """
    ä½¿ç”¨mask tokenè®¡ç®—å€™é€‰è¯çš„ç²¾ç¡®æŸå¤±ï¼ˆæ¨èæ–¹æ³•ï¼‰
    æ”¯æŒå•è¯å’Œå¤šè¯çŸ­è¯­ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£è‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬
    
    Args:
        text_with_mask: åŒ…å«mask tokençš„å¥å­
        candidate_word: å€™é€‰è¯æˆ–çŸ­è¯­
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        max_length: æœ€å¤§tokené•¿åº¦
        stride: æ»‘åŠ¨çª—å£æ­¥é•¿
        
    Returns:
        float: losså€¼ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    """
    # 1. å…ˆtokenizeå€™é€‰è¯ï¼Œçœ‹çœ‹å®ƒè¢«åˆ†æˆå¤šå°‘ä¸ªtoken
    candidate_tokens = tokenizer.tokenize(candidate_word)
    candidate_ids = tokenizer.convert_tokens_to_ids(candidate_tokens)
    
    # 2. æ ¹æ®å€™é€‰è¯çš„tokenæ•°é‡ï¼Œåˆ›å»ºå¯¹åº”æ•°é‡çš„mask
    if len(candidate_tokens) == 1:
        # å•ä¸ªtokençš„æƒ…å†µï¼Œä½¿ç”¨ç²¾ç¡®çš„maskæ–¹æ³•
        inputs = tokenizer(
            text_with_mask,
            return_tensors="pt",
            truncation=True,
            max_length=max_length,
            stride=stride,
            return_overflowing_tokens=True,
            padding=True
        )
        
        # æ‰¾åˆ°åŒ…å«mask tokençš„é‚£ä¸ªchunk
        mask_token_id = tokenizer.mask_token_id
        chunk_idx = 0
        
        # å¦‚æœæœ‰å¤šä¸ªchunksï¼Œæ‰¾åˆ°åŒ…å«maskçš„é‚£ä¸ª
        if "overflow_to_sample_mapping" in inputs:
            for idx in range(len(inputs["input_ids"])):
                if mask_token_id in inputs["input_ids"][idx]:
                    chunk_idx = idx
                    break
        
        # æ„é€ labelsï¼šåªåœ¨maskä½ç½®è®¡ç®—loss
        labels = torch.full_like(inputs["input_ids"], fill_value=-100)
        mask_positions = (inputs["input_ids"][chunk_idx] == mask_token_id).nonzero(as_tuple=True)[0]
        
        if len(mask_positions) > 0:
            labels[chunk_idx, mask_positions[0]] = candidate_ids[0]
        
        # åªä½¿ç”¨åŒ…å«maskçš„é‚£ä¸ªchunk
        selected_inputs = {
            "input_ids": inputs["input_ids"][chunk_idx:chunk_idx+1],
            "attention_mask": inputs["attention_mask"][chunk_idx:chunk_idx+1],
            "labels": labels[chunk_idx:chunk_idx+1]
        }
        
    else:
        # å¤šä¸ªtokençš„æƒ…å†µï¼Œç”¨å€™é€‰è¯æ›¿æ¢maskï¼Œè®¡ç®—æ•´ä½“loss
        text_with_candidate = text_with_mask.replace(tokenizer.mask_token, candidate_word)
        inputs = tokenizer(
            text_with_candidate,
            return_tensors="pt",
            truncation=True,
            max_length=max_length,
            stride=stride,
            return_overflowing_tokens=True,
            padding=True
        )
        
        # å¯¹äºå¤šè¯çŸ­è¯­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªchunkï¼ˆé€šå¸¸å€™é€‰è¯åœ¨å¼€å§‹éƒ¨åˆ†ï¼‰
        selected_inputs = {
            "input_ids": inputs["input_ids"][0:1],
            "attention_mask": inputs["attention_mask"][0:1],
            "labels": inputs["input_ids"][0:1].clone()
        }
    
    # å°†æ•°æ®é€å…¥è®¾å¤‡
    selected_inputs = {k: v.to(device) for k, v in selected_inputs.items()}
    
    # è®¡ç®—loss
    with torch.no_grad():
        outputs = model(**selected_inputs)
        return outputs.loss.item()


def clean_text(text: str, target_marker: str, option: str) -> str:
    """
    æ¸…ç†å’Œæ›¿æ¢æ–‡æœ¬ä¸­çš„æ ‡è®°
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        target_marker: ç›®æ ‡æ ‡è®°ï¼ˆå¦‚ "__1__"ï¼‰
        option: è¦å¡«å…¥çš„é€‰é¡¹
        
    Returns:
        str: æ¸…ç†åçš„æ–‡æœ¬
    """
    # æ›¿æ¢ç›®æ ‡æ ‡è®°
    current_test_text = text.replace(target_marker, option)
    # å°†å…¶ä»–é¢˜å·æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦
    clean_test_text = re.sub(r"__\d+__", "___", current_test_text)
    return clean_test_text


def crop_context(
    text: str,
    marker: str,
    context_size: int = 800
) -> str:
    """
    è£å‰ªæ–‡æœ¬ï¼Œä¿ç•™ç›®æ ‡æ ‡è®°å‘¨å›´çš„ä¸Šä¸‹æ–‡
    
    Args:
        text: å®Œæ•´æ–‡æœ¬
        marker: ç›®æ ‡æ ‡è®°
        context_size: ä¸Šä¸‹æ–‡å¤§å°ï¼ˆå‰åå„å–çš„å­—ç¬¦æ•°ï¼‰
        
    Returns:
        str: è£å‰ªåçš„æ–‡æœ¬
    """
    marker_pos = text.find(marker)
    if marker_pos == -1:
        return text
    
    start_pos = max(0, marker_pos - context_size)
    end_pos = min(len(text), marker_pos + context_size)
    return text[start_pos:end_pos]


def autoregressive_cloze_test(
    raw_text: str,
    options_dict: Dict[int, List[str]],
    tokenizer,
    model,
    device: torch.device,
    start_idx: int = 1,
    end_idx: int = 20,
    context_size: int = 800,  # ä¿ç•™å‚æ•°ä»¥å‘åå…¼å®¹ï¼Œä½†ä¸å†ä½¿ç”¨
    max_length: int = 512,
    letters: Optional[List[str]] = None
) -> Dict[int, Dict]:
    """
    è‡ªå›å½’å®Œå½¢å¡«ç©ºç­”é¢˜
    ä½¿ç”¨tokenizerçš„æ»‘åŠ¨çª—å£è‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬
    
    Args:
        raw_text: åŸå§‹æ–‡æœ¬ï¼ˆåŒ…å« __n__ æ ‡è®°ï¼‰
        options_dict: é€‰é¡¹å­—å…¸ {é¢˜å·: [é€‰é¡¹åˆ—è¡¨]}
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        start_idx: èµ·å§‹é¢˜å·
        end_idx: ç»“æŸé¢˜å·ï¼ˆä¸åŒ…å«ï¼‰
        context_size: ï¼ˆå·²å¼ƒç”¨ï¼‰ä¿ç•™ä»¥å‘åå…¼å®¹ï¼Œtokenizerè‡ªåŠ¨å¤„ç†æ»‘åŠ¨çª—å£
        max_length: æœ€å¤§tokené•¿åº¦
        letters: é€‰é¡¹å­—æ¯åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["A", "B", "C", "D"]
        
    Returns:
        dict: {é¢˜å·: {"option": é€‰é¡¹, "letter": å­—æ¯, "loss": losså€¼}}
    """
    if letters is None:
        letters = ["A", "B", "C", "D"]
    
    print("\n--- å¼€å§‹å¸¦ GPU åŠ é€Ÿçš„è‡ªå›å½’ç­”é¢˜ ---")
    
    working_text = raw_text
    results_summary = {}
    
    for i in range(start_idx, end_idx + 1):
        if i not in options_dict:
            continue
            
        target_marker = f"__{i}__"
        results = {}
        
        # å°†ç›®æ ‡æ ‡è®°æ›¿æ¢ä¸º mask token
        text_with_mask = working_text.replace(target_marker, tokenizer.mask_token)
        # å°†å…¶ä»–é¢˜å·æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦
        clean_text_with_mask = re.sub(r"__\d+__", "___", text_with_mask)
        
        # ä¸å†éœ€è¦æ‰‹åŠ¨è£å‰ªï¼Œtokenizerä¼šè‡ªåŠ¨ç”¨æ»‘åŠ¨çª—å£å¤„ç†é•¿æ–‡æœ¬
        for opt in options_dict[i]:
            # ä½¿ç”¨æ–°çš„åŸºäºmaskçš„è¯„åˆ†æ–¹æ³•ï¼ˆè‡ªå¸¦æ»‘åŠ¨çª—å£ï¼‰
            loss = score_candidate_word(
                clean_text_with_mask,
                opt,
                tokenizer,
                model,
                device,
                max_length
            )
            
            results[opt] = loss
        
        # æ‰¾å‡º Loss æœ€ä½çš„é€‰é¡¹
        best_opt = min(results, key=results.get)
        best_loss = results[best_opt]
        best_idx = options_dict[i].index(best_opt)
        best_letter = letters[best_idx]
        
        # å¡«å…¥ç­”æ¡ˆï¼Œå®ç°è‡ªå›å½’
        working_text = working_text.replace(target_marker, best_opt)
        
        # ä¿å­˜ç»“æœ
        results_summary[i] = {
            "option": best_opt,
            "letter": best_letter,
            "loss": best_loss,
            "all_scores": results
        }
        
        print(
            f"ç¬¬ {i:02d} é¢˜ -> æ¨¡å‹é€‰æ‹©: {best_letter}. {best_opt} (Loss: {best_loss:.4f})"
        )
    
    print("\n--- ç­”é¢˜ç»“æŸ ---")
    return results_summary


def simple_cloze_test(
    prompt_template: str,
    options: List[str],
    tokenizer,
    model,
    device: torch.device
) -> Dict[str, float]:
    """
    ç®€å•çš„å®Œå½¢å¡«ç©ºæµ‹è¯•ï¼ˆå•é¢˜ï¼‰
    
    Args:
        prompt_template: æç¤ºæ¨¡æ¿ï¼Œä½¿ç”¨ {} ä½œä¸ºå ä½ç¬¦
        options: é€‰é¡¹åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        dict: {é€‰é¡¹: losså€¼}ï¼ŒæŒ‰lossä»å°åˆ°å¤§æ’åº
    """
    print("\n--- å¼€å§‹å…è®­ç»ƒ(Zero-shot)æ‰“åˆ† ---")
    
    results = {}
    
    for opt in options:
        complete_sentence = prompt_template.format(opt)
        loss_score = score_sentence(complete_sentence, tokenizer, model, device)
        results[opt] = loss_score
    
    sorted_results = sorted(results.items(), key=lambda x: x[1])
    
    for rank, (word, loss) in enumerate(sorted_results, 1):
        if rank == 1:
            print(f"ğŸ† æœ€ä½³é€‰é¡¹ -> {word} (ä¸è‡ªç„¶åº¦ Loss: {loss:.4f})")
        else:
            print(f"   æ·˜æ±°é€‰é¡¹ -> {word} (ä¸è‡ªç„¶åº¦ Loss: {loss:.4f})")
    
    return dict(sorted_results)


def print_results_table(
    results: Dict[int, Dict],
    answers: Optional[Dict[int, str]] = None
) -> None:
    """
    æ‰“å°ç­”é¢˜ç»“æœè¡¨æ ¼
    
    Args:
        results: autoregressive_cloze_test è¿”å›çš„ç»“æœ
        answers: æ ‡å‡†ç­”æ¡ˆå­—å…¸ {é¢˜å·: "å­—æ¯"}
    """
    print("\n| é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ |")
    print("|---:|:---|:---|:---:|")
    
    for i in sorted(results.keys()):
        model_choice = f"{results[i]['letter']}. {results[i]['option']}"
        
        if answers and i in answers:
            is_correct = results[i]['letter'] == answers[i]
            mark = "âœ…" if is_correct else "âŒ"
            print(f"| {i:02d} | {model_choice} | {answers[i]} | {mark} |")
        else:
            print(f"| {i:02d} | {model_choice} | - | - |")

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# # è€ƒç ”è‹±è¯­åˆ°åº•æœ‰å¤šå˜æ€ï¼ŸBERTåªç­”å¯¹ä¸€åŠ

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## BERT ç®€å•é¢˜ç›®æµ‹è¯•

# %% [code] {"execution":{"iopub.status.busy":"2026-02-26T05:38:21.717368Z","iopub.execute_input":"2026-02-26T05:38:21.717762Z","iopub.status.idle":"2026-02-26T05:38:31.230132Z","shell.execute_reply.started":"2026-02-26T05:38:21.717737Z","shell.execute_reply":"2026-02-26T05:38:31.229262Z"},"jupyter":{"outputs_hidden":false}}
# from bert_utils import load_model, simple_cloze_test

# åŠ è½½æ¨¡å‹
tokenizer, model, device = load_model("roberta-base")

# é¢˜ç›®å’Œå¤š Token é€‰é¡¹
prompt_template = (
    "Despite the {} evidence, the jury found it difficult to reach a unanimous verdict."
)
options = ["overwhelming", "vague", "insufficient", "unreliable"]

# æ‰§è¡Œç®€å•æµ‹è¯•
results = simple_cloze_test(prompt_template, options, tokenizer, model, device)

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## BERT 2022å¹´è€ƒç ”è‹±è¯­ä¸€å®Œå½¢å¡«ç©º

# %% [code] {"execution":{"iopub.status.busy":"2026-02-26T05:38:31.231085Z","iopub.execute_input":"2026-02-26T05:38:31.231309Z","iopub.status.idle":"2026-02-26T05:38:34.644040Z","shell.execute_reply.started":"2026-02-26T05:38:31.231288Z","shell.execute_reply":"2026-02-26T05:38:34.643345Z"},"jupyter":{"outputs_hidden":false}}
#from bert_utils import autoregressive_cloze_test, load_model

# åŠ è½½æ¨¡å‹
tokenizer, model, device = load_model("roberta-base")

# åŸå§‹æ–‡æœ¬
raw_text = """
The idea that plants have some degree of consciousness first took root in the early 2000s; the term â€œplant neurobiologyâ€ was __1__ around the notion that some aspects of plant behavior could be __2__ to intelligence in animals. __3__ plants lack brains, the firing of electrical signals in their stems and leaves nonetheless triggered responses that __4__ consciousness, researchers previously reported.

But such an idea is untrue, according to a new opinion article. Plant biology is complex and fascinating, but it __5__ so greatly from that of animals that so-called __6__ of plantsâ€™ intelligence is inconclusive, the authors wrote.

Beginning in 2006, some scientists have __7__ that plants possess neuron-like cells that interact with hormones and neurotransmitters, __8__ â€œa plant nervous system, __9__ to that in animals,â€ said lead study author Lincoln Taiz, â€œThey __10__ claimed that plants have â€œbrain-like command centersâ€ at their root tips.â€

This __11__ makes sense if you simplify the workings of a complex brain, __12__ it to an array of electrical pulses; cells in plants also communicate through electrical signals. __13__, the signaling in a plant is only __14__ similar to the firing in a complex animal brain, which is more than â€œa mass of cells that communicate by electricity,â€ Taiz said.

â€œFor consciousness to evolve, a brain with a threshold __15__ of complexity and capacity is required,â€ he __16__.â€Since plants donâ€™t have nervous systems, the __17__ that they have consciousness are effectively zero.â€

And whatâ€™s so great about consciousness, anyway? Plants canâ€™t run away from __18__, so investing energy in a body system which __19__ a threat and can feel pain would be a very __20__ evolutionary strategy, according to the article.
"""

# é€‰é¡¹å­—å…¸
options_dict = {
    1: ["coined", "discovered", "collected", "issued"],
    2: ["attributed", "directed", "compared", "confined"],
    3: ["unless", "when", "once", "though"],
    4: ["coped with", "consisted of", "hinted at", "extended"],
    5: ["suffers", "benefits", "develops", "differs"],
    6: ["acceptance", "evidence", "cultivation", "creation"],
    7: ["doubted", "denied", "argued", "requested"],
    8: ["adapting", "forming", "repairing", "testing"],
    9: ["analogous", "essential", "suitable", "sensitive"],
    10: ["just", "ever", "still", "even"],
    11: ["restriction", "experiment", "perspective", "demand"],
    12: ["attaching", "reducing", "returning", "exposing"],
    13: ["However", "Moreover", "Therefore", "Otherwise"],
    14: ["temporarily", "literally", "superficially", "imaginarily"],
    15: ["list", "level", "label", "local"],
    16: ["recalled", "agreed", "questioned", "added"],
    17: ["chances", "risks", "excuses", "assumptions"],
    18: ["danger", "failure", "warning", "control"],
    19: ["represents", "includes", "reveals", "recognizes"],
    20: ["humble", "poor", "practical", "easy"],
}

# æ‰§è¡Œè‡ªå›å½’ç­”é¢˜
results1 = autoregressive_cloze_test(
    raw_text, options_dict, tokenizer, model, device,
    start_idx=1, end_idx=20
)

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# | é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ |
# |---:|:---|:---|:---:|
# | 01 | A. coined | A. coined | âœ… |
# | 02 | C. compared | C. compared | âœ… |
# | 03 | D. though | D. though | âœ… |
# | 04 | A. coped with | C. hinted at | âŒ |
# | 05 | A. suffers | D. differs | âŒ |
# | 06 | B. evidence | B. evidence | âœ… |
# | 07 | B. denied | C. argued | âŒ |
# | 08 | B. forming | B. forming | âœ… |
# | 09 | A. analogous | A. analogous | âœ… |
# | 10 | A. just | D. even | âŒ |
# | 11 | B. experiment | C. perspective | âŒ |
# | 12 | A. attaching | B. reducing | âŒ |
# | 13 | C. Therefore | A. However | âŒ |
# | 14 | C. superficially | C. superficially | âœ… |
# | 15 | B. level | B. level | âœ… |
# | 16 | B. agreed | D. added | âŒ |
# | 17 | D. assumptions | A. chances | âŒ |
# | 18 | A. danger | A. danger | âœ… |
# | 19 | A. represents | D. recognizes | âŒ |
# | 20 | B. poor | B. poor | âœ… |

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## BERT 2023å¹´è€ƒç ”è‹±è¯­ä¸€å®Œå½¢å¡«ç©º

# %% [code] {"execution":{"iopub.status.busy":"2026-02-26T05:38:34.645176Z","iopub.execute_input":"2026-02-26T05:38:34.645637Z","iopub.status.idle":"2026-02-26T05:38:38.021953Z","shell.execute_reply.started":"2026-02-26T05:38:34.645586Z","shell.execute_reply":"2026-02-26T05:38:38.021276Z"},"jupyter":{"outputs_hidden":false}}
#from bert_utils import autoregressive_cloze_test, load_model

# åŠ è½½æ¨¡å‹
tokenizer, model, device = load_model("roberta-base")

# ä¿®å¤ OCR ä¹±ç åçš„çº¯å‡€æ–‡æœ¬
raw_text = """
Caravanserais were roadside inns that were built along the Silk Road in areas including China, North Africa and the Middle East. They were typically __1__ outside the walls of a city or village and were usually funded by governments or __2__. This word â€œCaravanseraisâ€ is a __3__ of the Persian word â€œkarvanâ€, which means a group of travellers or a caravan, and seray, a palace or enclosed building. The term caravan was used to __4__ groups of people who travelled together across the ancient network for safety reasons, __5__ merchants, travellers or pilgrims. From the 10th century onwards, as merchant and travel routes become more developed, the __6__ of the Caravanserais increased and they served as a safe place for people to rest at night. Travellers on the Silk Road __7__ the possibility of being attacked by thieves or being __8__ to extreme conditions. For this reason, Caravanserais were strategically placed __9__ they could be reached in a dayâ€™s travel time. Caravanserais served as an informal __10__ point for the various people who travelled the Silk Road. __11__, those structures became important centers for culture __12__ and interaction, with travelers sharing their cultures, ideas and beliefs, __13__ taking knowledge with them, greatly __14__ the development of several civilizations. Caravanserais were also an important marketplace for commodities and __15__ in the trade of goods along the Silk Road. __16__, it was frequently the first stop for merchants looking to sell their wares and __17__ supplies for their own journeys. It is __18__ that around 12,000 to 15,000 caravanserais were built along the Silk Road, __19__ only about 3,000 are known to remain today, many of which are in __20__.
"""

# é€‰é¡¹å­—å…¸
options_dict = {
    1: ["displayed", "occupied", "located", "equipped"],
    2: ["privately", "regularly", "respectively", "permanently"],
    3: ["definition", "transition", "substitution", "combination"],
    4: ["classify", "record", "describe", "connect"],
    5: ["apart from", "instead of", "such as", "along with"],
    6: ["construction", "restoration", "impression", "evaluation"],
    7: ["doubted", "faced", "accepted", "reduced"],
    8: ["assigned", "subjected", "accustomed", "opposed"],
    9: ["so that", "even if", "now that", "in case"],
    10: ["talking", "starting", "breaking", "meeting"],
    11: ["By the way", "On occasion", "In comparison", "As a result"],
    12: ["heritage", "revival", "exchange", "status"],
    13: ["with regard to", "in spite of", "as well as", "in line with"],
    14: ["completing", "influencing", "resuming", "pioneering"],
    15: ["aided", "invested", "failed", "competed"],
    16: ["Rather", "Indeed", "Otherwise", "However"],
    17: ["go in for", "stand up for", "close in on", "stock up on"],
    18: ["believed", "predicted", "recalled", "implied"],
    19: ["until", "because", "unless", "although"],
    20: ["ruins", "debt", "fashion", "series"],
}

# æ‰§è¡Œè‡ªå›å½’ç­”é¢˜
results2 = autoregressive_cloze_test(
    raw_text, options_dict, tokenizer, model, device,
    start_idx=1, end_idx=20
)

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# | é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ |
# |---:|:---|:---|:---:|
# | 01 | C. located | C. located | âœ… |
# | 02 | A. privately | A. privately | âœ… |
# | 03 | A. definition | D. combination | âŒ |
# | 04 | C. describe | C. describe | âœ… |
# | 05 | B. instead of | C. such as | âŒ |
# | 06 | A. construction | A. construction | âœ… |
# | 07 | D. reduced | B. faced | âŒ |
# | 08 | B. subjected | B. subjected | âœ… |
# | 08 | B. predicted | A. believed | âŒ |
# | 09 | B. even if | A. so that | âŒ |
# | 10 | B. starting | D. meeting | âŒ |
# | 11 | A. By the way | D. As a result | âŒ |
# | 12 | B. revival | C. exchange | âŒ |
# | 13 | A. with regard to | C. as well as | âŒ |
# | 14 | C. resuming | B. influencing | âŒ |
# | 15 | A. aided | A. aided | âœ… |
# | 16 | D. However | B. Indeed | âŒ |
# | 17 | A. go in for | D. stock up on | âŒ |
# | 19 | D. although | D. although | âœ… |
# | 20 | A. ruins | A. ruins | âœ… |

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## BERT 2019å¹´ä¸Šæµ·è‹±è¯­é«˜è€ƒå®Œå½¢å¡«ç©º

# %% [code] {"execution":{"iopub.status.busy":"2026-02-26T05:38:38.022882Z","iopub.execute_input":"2026-02-26T05:38:38.023117Z","iopub.status.idle":"2026-02-26T05:38:41.531809Z","shell.execute_reply.started":"2026-02-26T05:38:38.023094Z","shell.execute_reply":"2026-02-26T05:38:41.530995Z"},"jupyter":{"outputs_hidden":false}}
#from bert_utils import autoregressive_cloze_test, load_model

# åŠ è½½æ¨¡å‹
tokenizer, model, device = load_model("roberta-base")

# çº¯å‡€åŸæ–‡æœ¬ (1-15é¢˜)
raw_text = """
We're told that writing is dying. Typing on keyboards and screens __1__ written communication today. Learning cursive, joined-up handwriting was once __2__ in schools. But now, not so much. Countries such as Finland have dropped joined-up handwriting lessons in __3__ of typing courses. And in the US, the requirement to learn cursive has been left out of core standards since 2013. A few US states still place value on formative cursive education, such as Arizona, but they're not the __4__.

Some experts point out that writing lessons can have indirect __5__. Anne Trubek, author of The History and Uncertain Future of Handwriting, argues that such lessons can reinforce a skill called automaticity. That's when you've perfected a task, and can do it almost without thinking, __6__ you extra mental bandwidth to think about other things while you're doing the task. In this sense, Trubek likens handwriting to __7__.

"Once you have driven for a while, you don't __8__ think 'Step on gas now' [or] 'Turn the steering wheel a bit'," she explains. "You just do it. That's what we want children to __9__ when learning to write. You don't think 'now make a loop going up for the 't'' or 'now look for the letter 'r' on the keyboard'."

Trubek has written many essays and books on handwriting, and she doesn't believe it will die out for a very long time, "ever", but she believes students are learning how to type faster without looking at the keys at __10__ ages, and students are learning automaticity with keyboards that was once exclusive to handwriting: to type faster than they could write, granting them extra time to think about word choice or sentence structure. In a piece penned for the New York Times last year, Trubek argued that due to the improved automaticity of keyboards, today's children may well become better communicators in text, as __11__ take up less of their education. 

This is a(n) __12__ that has attracted both criticism and support. She explains that two of the most common arguments she hears from detractors regarding the decline of handwriting is that not __13__ it will result in a loss of history and a "loss of personal touch".

On the former she __14__ that 95% of handwritten manuscripts can't be read by the average person anyway â€“ "that's why we have paleographers," she explains, paleography being the study of ancient styles of writing â€“ while the latter refers to the warm __15__ we give to handwritten personal notes, such as thank-you cards.
"""

# é€‰é¡¹å­—å…¸ (1-15é¢˜)
options_dict = {
    1: ["abandons", "dominates", "enters", "absorbs"],
    2: ["compulsory", "opposite", "crucial", "relevant"],
    3: ["in want of", "in case of", "in favour of", "in addition to"],
    4: ["quantity", "minimum", "quality", "majority"],
    5: ["responsibility", "benefits", "resources", "structure"],
    6: ["granting", "getting", "bringing", "costing"],
    7: ["sleeping", "driving", "reviewing", "operating"],
    8: ["eventually", "constantly", "frequently", "consciously"],
    9: ["adopt", "reach", "acquire", "activate"],
    10: ["slower", "later", "faster", "earlier"],
    11: ["handwriting", "typing", "reading", "spelling"],
    12: ["trust", "book", "view", "smile"],
    13: ["containing", "spreading", "choosing", "preserving"],
    14: ["commits", "counters", "completes", "composes"],
    15: ["associations", "resources", "procedures", "interactions"],
}

# æ‰§è¡Œè‡ªå›å½’ç­”é¢˜
results3 = autoregressive_cloze_test(
    raw_text, options_dict, tokenizer, model, device,
    start_idx=1, end_idx=15
)

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# | é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ | é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ |
# |---|---|---|---|---|---|---|---|
# | 01 | A. abandons | B. dominates | âŒ | 09 | A. adopt | C. acquire | âŒ |
# | 02 | C. crucial | A. compulsory | âŒ | 10 | D. earlier | D. earlier | âœ… |
# | 03 | C. in favour of | C. in favour of | âœ… | 11 | A. handwriting | A. handwriting | âœ… |
# | 04 | B. minimum | D. majority | âŒ | 12 | C. view | C. view | âœ… |
# | 05 | D. structure | B. benefits | âŒ | 13 | D. preserving | D. preserving | âœ… |
# | 06 | A. granting | A. granting | âœ… | 14 | D. composes | B. counters | âŒ |
# | 07 | B. driving | B. driving | âœ… | 15 | A. associations | A. associations | âœ… |
# | 08 | D. consciously | D. consciously | âœ… |  |  |  |  |

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## BERT 2019å¹´ä¸Šæµ·è‹±è¯­æ˜¥è€ƒå®Œå½¢å¡«ç©º

# %% [code] {"execution":{"iopub.status.busy":"2026-02-26T05:38:41.533835Z","iopub.execute_input":"2026-02-26T05:38:41.534155Z","iopub.status.idle":"2026-02-26T05:38:44.815548Z","shell.execute_reply.started":"2026-02-26T05:38:41.534130Z","shell.execute_reply":"2026-02-26T05:38:44.814758Z"},"jupyter":{"outputs_hidden":false}}
#from bert_utils import autoregressive_cloze_test, load_model

# åŠ è½½æ¨¡å‹
tokenizer, model, device = load_model("roberta-base")

# çº¯äººå·¥ç²¾æ ¡å½•å…¥çš„åŸæ–‡
raw_text = """
More people are travelling than ever before, and lower barriers to entry and falling costs means they are doing so for __41__ periods.

The rise of "city breaks" 48-hour bursts of foreign cultures, easier on the pocket and annual leave balance has increased tourist numbers, but not their __42__ spread. The same attractions have been used to market cities such as Paris, Barcelona and Venice for decades, and visitors use the same infrastructure as residents to reach them. "Too many people do the same thing at the exact same time," says Font. "For __43__, the city no longer belongs to them."

This starts with marketing, says Font, who notes that Amsterdam has started advising visitors to seek __44__ outside of the city center on its official website. "That takes some balls, really, to do that. But only so many people will look at the website, and it means they can say to their residents they're doing all they can (to ease congestion)."

But it also __45__ a better way, it is calling "de-tourism": sustainable travel tips and __46__ itineraries for exploring an authentic Venice, off the paths beaten by the 28 million visitors who flock there each year.

A greater variety of __47__ for prospective visitorsâ€”ideas for what to do in off-peak seasons, for example, or outside of the city centerâ€”can have the effect of diverting them from already saturated landmarks, or __48__ short breaks away in the first place.

Longer stays __49__ the pressure, says Font. "If you go to Paris for two days, you're not going to go to the Eiffel Tower. If you go for two weeks, you're not going to go to the Eiffel tower 14 times."

Similarly, repeat visitors have a better sense of the __50__, "We should be asking how do we get tourists to __51__, not how to get them to come for the first time. If they're coming for the fifth time, it is much easier to integrate their behavior with ours."

Local governments can foster this sustainable activity by giving preference to responsible operators and even high-paying consumers. Font says cities could stand to be more selective about the tourists they try to attract when the current metric for marketing success is how many there are, and how far they've come. "You're thinking, 'yeah but at what cost...'"

He points to unpublished data from the Barcelona Tourist Board that prioritizes Japanese tourists for spending an average of 640 more per day than French touristsâ€”a(n) __52__ that fails to take into account their bigger carbon footprint. __53__ tourists are also more likely to be repeat visitors that come at off-peak times, buy local product, and __54__ less crowded parts of the cityâ€”all productive steps towards more __55__ and more peaceful relations with residents.
"""

# é€‰é¡¹å­—å…¸ (41 - 55é¢˜)
options_dict = {
    41: ["longer", "shorter", "wider", "clearer"],
    42: ["environmental", "national", "economic", "geographic"],
    43: ["locals", "tourists", "visitors", "cleaners"],
    44: ["transports", "accommodation", "restaurants", "service"],
    45: ["addresses", "introduces", "proposes", "receives"],
    46: ["separate", "individual", "alternative", "objective"],
    47: ["reform", "guidance", "invitation", "support"],
    48: ["convincing", "discouraging", "preventing", "resisting"],
    49: ["peace", "risk", "leisure", "ease"],
    50: ["culture", "knowledge", "entertainment", "ability"],
    51: ["go with", "bring up", "come back", "lay off"],
    52: ["distinction", "harmony", "association", "comparison"],
    53: ["French", "Italian", "Spanish", "German"],
    54: ["carry out", "give into", "spread out", "impact on"],
    55: ["sight", "complex", "temporary", "sustainable"],
}

# æ‰§è¡Œè‡ªå›å½’ç­”é¢˜
results4 = autoregressive_cloze_test(
    raw_text, options_dict, tokenizer, model, device,
    start_idx=41, end_idx=55
)

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# | é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ | é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ |
# |---|---|---|---|---|---|---|---|
# | 41 | B. shorter | B. shorter | âœ… | 49 | D. ease | D. ease | âœ… |
# | 42 | B. national | D. geographic | âŒ | 50 | B. knowledge | A. culture | âŒ |
# | 43 | A. locals | A. locals | âœ… | 51 | C. come back | C. come back | âœ… |
# | 44 | B. accommodation | B. accommodation | âœ… | 52 | A. distinction | D. comparison | âŒ |
# | 45 | B. introduces | C. proposes | âŒ | 53 | C. Spanish | A. French | âŒ |
# | 46 | C. alternative | C. alternative | âœ… | 54 | C. spread out | C. spread out | âœ… |
# | 47 | B. guidance | B. guidance | âœ… | 55 | B. complex | D. sustainable | âŒ |
# | 48 | C. preventing | B. discouraging | âŒ |  |  |  |  |

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## å‡†ç¡®ç‡è¯„ä¼°

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-26T05:38:44.816512Z","iopub.execute_input":"2026-02-26T05:38:44.816785Z","iopub.status.idle":"2026-02-26T05:38:44.822089Z","shell.execute_reply.started":"2026-02-26T05:38:44.816761Z","shell.execute_reply":"2026-02-26T05:38:44.821266Z"}}
def calculate_accuracy(results, correct_answers):
    """
    è®¡ç®—ç­”é¢˜å‡†ç¡®ç‡
    
    å‚æ•°:
        results: autoregressive_cloze_test è¿”å›çš„ç»“æœå­—å…¸ {é¢˜å·: {"letter": "A", ...}}
        correct_answers: å­—å…¸ï¼Œé”®ä¸ºé¢˜å·ï¼Œå€¼ä¸ºæ­£ç¡®ç­”æ¡ˆçš„ç´¢å¼•(0=A, 1=B, 2=C, 3=D)
    
    è¿”å›:
        accuracy: å‡†ç¡®ç‡ (0-1ä¹‹é—´çš„æµ®ç‚¹æ•°)
        correct_count: æ­£ç¡®é¢˜ç›®æ•°
        total_count: æ€»é¢˜ç›®æ•°
    """
    letter_to_idx = {"A": 0, "B": 1, "C": 2, "D": 3}
    correct_count = 0
    total_count = len(results)
    
    for question_num, result_dict in results.items():
        if question_num in correct_answers:
            # ä» letter è½¬æ¢ä¸ºç´¢å¼•
            selected_letter = result_dict['letter']
            selected_idx = letter_to_idx[selected_letter]
            
            if selected_idx == correct_answers[question_num]:
                correct_count += 1
    
    accuracy = correct_count / total_count if total_count > 0 else 0
    return accuracy, correct_count, total_count

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ### å®šä¹‰æ ‡å‡†ç­”æ¡ˆ

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-26T05:38:44.823104Z","iopub.execute_input":"2026-02-26T05:38:44.823399Z","iopub.status.idle":"2026-02-26T05:38:44.847909Z","shell.execute_reply.started":"2026-02-26T05:38:44.823368Z","shell.execute_reply":"2026-02-26T05:38:44.847221Z"}}
# 2022å¹´è€ƒç ”è‹±è¯­ä¸€ç­”æ¡ˆ (0=A, 1=B, 2=C, 3=D)
answers_2022 = {
    1: 0,   # A. coined
    2: 2,   # C. compared
    3: 3,   # D. though
    4: 2,   # C. hinted at
    5: 3,   # D. differs
    6: 1,   # B. evidence
    7: 2,   # C. argued
    8: 1,   # B. forming
    9: 0,   # A. analogous
    10: 3,  # D. even
    11: 2,  # C. perspective
    12: 1,  # B. reducing
    13: 0,  # A. However
    14: 2,  # C. superficially
    15: 1,  # B. level
    16: 3,  # D. added
    17: 0,  # A. chances
    18: 0,  # A. danger
    19: 3,  # D. recognizes
    20: 1,  # B. poor
}

# 2023å¹´è€ƒç ”è‹±è¯­ä¸€ç­”æ¡ˆ (Caravanseraisé©¿ç«™æ–‡ç« )
answers_2023 = {
    1: 2,   # C. located
    2: 0,   # A. privately
    3: 3,   # D. combination
    4: 2,   # C. describe
    5: 2,   # C. such as
    6: 0,   # A. construction
    7: 1,   # B. faced
    8: 1,   # B. subjected
    9: 0,   # A. so that
    10: 3,  # D. meeting
    11: 3,  # D. As a result
    12: 2,  # C. exchange
    13: 2,  # C. as well as
    14: 1,  # B. influencing
    15: 0,  # A. aided
    16: 1,  # B. Indeed
    17: 3,  # D. stock up on
    18: 0,  # A. believed
    19: 3,  # D. although
    20: 0,  # A. ruins
}

# 2019å¹´ä¸Šæµ·è‹±è¯­é«˜è€ƒç­”æ¡ˆ (handwritingæ–‡ç« ï¼Œé¢˜å·1-15)
answers_2019_gaokao = {
    1: 1,   # B. dominates
    2: 0,   # A. compulsory
    3: 2,   # C. in favour of
    4: 3,   # D. majority
    5: 1,   # B. benefits
    6: 0,   # A. granting
    7: 1,   # B. driving
    8: 3,   # D. consciously
    9: 2,   # C. acquire
    10: 3,  # D. earlier
    11: 0,  # A. handwriting
    12: 2,  # C. view
    13: 3,  # D. preserving
    14: 1,  # B. counters
    15: 0,  # A. associations
}

# 2019å¹´æ˜¥å­£é«˜è€ƒç­”æ¡ˆ
answers_2019_spring = {
    41: 1,  # B. shorter
    42: 3,  # D. geographic
    43: 0,  # A. locals
    44: 1,  # B. accommodation
    45: 2,  # C. proposes
    46: 2,  # C. alternative
    47: 1,  # B. guidance
    48: 1,  # B. discouraging
    49: 3,  # D. ease
    50: 0,  # A. culture
    51: 2,  # C. come back
    52: 3,  # D. comparison
    53: 0,  # A. French
    54: 2,  # C. spread out
    55: 3,  # D. sustainable
}

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ### è®¡ç®—å„æµ‹è¯•å‡†ç¡®ç‡

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-26T05:38:44.848889Z","iopub.execute_input":"2026-02-26T05:38:44.849177Z","iopub.status.idle":"2026-02-26T05:38:44.868013Z","shell.execute_reply.started":"2026-02-26T05:38:44.849148Z","shell.execute_reply":"2026-02-26T05:38:44.867432Z"}}
# è®¡ç®—æ‰€æœ‰4ä¸ªæµ‹è¯•çš„å‡†ç¡®ç‡
print("=" * 70)
print("BERT å®Œå½¢å¡«ç©ºå‡†ç¡®ç‡è¯„ä¼°æŠ¥å‘Š".center(70))
print("=" * 70)
print()

# æµ‹è¯•1: 2022å¹´è€ƒç ”è‹±è¯­ä¸€
accuracy_2022, correct_2022, total_2022 = calculate_accuracy(results1, answers_2022)
print(f"ğŸ“ 2022å¹´è€ƒç ”è‹±è¯­ä¸€:  {correct_2022:2d}/{total_2022:2d} = {accuracy_2022:6.1%}")

# æµ‹è¯•2: 2023å¹´è€ƒç ”è‹±è¯­ä¸€
accuracy_2023, correct_2023, total_2023 = calculate_accuracy(results2, answers_2023)
print(f"ğŸ“ 2023å¹´è€ƒç ”è‹±è¯­ä¸€:  {correct_2023:2d}/{total_2023:2d} = {accuracy_2023:6.1%}")

# æµ‹è¯•3: 2019å¹´ä¸Šæµ·é«˜è€ƒ
accuracy_2019_gaokao, correct_2019_gaokao, total_2019_gaokao = calculate_accuracy(results3, answers_2019_gaokao)
print(f"ğŸ“ 2019å¹´ä¸Šæµ·é«˜è€ƒ:    {correct_2019_gaokao:2d}/{total_2019_gaokao:2d} = {accuracy_2019_gaokao:6.1%}")

# æµ‹è¯•4: 2019å¹´æ˜¥å­£é«˜è€ƒ
accuracy_2019_spring, correct_2019_spring, total_2019_spring = calculate_accuracy(results4, answers_2019_spring)
print(f"ğŸ“ 2019å¹´æ˜¥å­£é«˜è€ƒ:    {correct_2019_spring:2d}/{total_2019_spring:2d} = {accuracy_2019_spring:6.1%}")

print()
print("-" * 70)

# æ€»ä½“ç»Ÿè®¡
total_correct = correct_2022 + correct_2023 + correct_2019_gaokao + correct_2019_spring
total_questions = total_2022 + total_2023 + total_2019_gaokao + total_2019_spring
total_accuracy = total_correct / total_questions if total_questions > 0 else 0

print(f"ğŸ¯ æ€»ä½“å‡†ç¡®ç‡:        {total_correct:2d}/{total_questions:2d} = {total_accuracy:6.1%}")
print("=" * 70)

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ## ç»“è®º

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# #### è€ƒç ”çš„ç”±äºå®ƒçš„ä¸Šä¸‹æ–‡é•¿åº¦åœ¨512tokenä¹‹å†…æ‰€ä»¥ä¸éœ€è¦åˆ‡æ‰ä¸Šä¸‹æ–‡ è€Œ é«˜è€ƒè‹±è¯­éœ€è¦

# %% [markdown] {"jupyter":{"outputs_hidden":false}}
# ### çº¯åº•å±‚çš„è¯­è¨€æ¨¡å‹åªæ˜¯â€œæ¦‚ç‡çš„å¥´éš¶â€å’Œâ€œè¯­æ„Ÿå¤§å¸ˆâ€ï¼Œåªæœ‰è·¨è¶Šäº†ä»â€œç»Ÿè®¡é«˜é¢‘è¯æ‹¼å‡‘â€åˆ°â€œä¸Šä¸‹æ–‡å› æœæ¨ç†â€çš„é¸¿æ²Ÿï¼ˆæ¯”å¦‚å¼•å…¥å¾®è°ƒã€æ ‘æ¨¡å‹æˆ–æ€ç»´é“¾ï¼‰ï¼ŒAI æ‰èƒ½çœŸæ­£è¯»æ‡‚äººç±»çš„å¤æ‚é€»è¾‘ã€‚