"""
BERTå®Œå½¢å¡«ç©ºæµ‹è¯•å…¬å…±å‡½æ•°åº“
æä¾›æ¨¡å‹åŠ è½½ã€è¯„åˆ†ã€è‡ªå›å½’ç­”é¢˜ç­‰é€šç”¨åŠŸèƒ½
"""

import re
from typing import Dict, List, Optional, Tuple

import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer


def setup_device() -> torch.device:
    """
    è®¾ç½®è®¡ç®—è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    
    Returns:
        torch.device: å¯ç”¨çš„è®¡ç®—è®¾å¤‡
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"å½“å‰è®¡ç®—è®¾å¤‡: {device}")
    return device


def load_model(model_name: str = "roberta-base", device: Optional[torch.device] = None) -> Tuple:
    """
    åŠ è½½é¢„è®­ç»ƒçš„MLMæ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "roberta-base"
        device: è®¡ç®—è®¾å¤‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
        
    Returns:
        tuple: (tokenizer, model, device)
    """
    if device is None:
        device = setup_device()
    
    print(f"æ­£åœ¨åŠ è½½ {model_name} åˆ°æ˜¾å­˜...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)
    model.eval()
    
    return tokenizer, model, device


def score_sentence(
    sentence: str,
    tokenizer,
    model,
    device: torch.device,
    max_length: int = 512
) -> float:
    """
    è®¡ç®—å¥å­çš„ä¸è‡ªç„¶åº¦ï¼ˆlosså€¼ï¼‰- æ—§æ–¹æ³•ï¼Œç”¨äºå‘åå…¼å®¹
    
    Args:
        sentence: å¾…è¯„åˆ†çš„å¥å­
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        max_length: æœ€å¤§tokené•¿åº¦
        
    Returns:
        float: losså€¼ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    """
    inputs = tokenizer(
        sentence,
        return_tensors="pt",
        truncation=True,
        max_length=max_length
    )
    inputs["labels"] = inputs["input_ids"].clone()
    
    # å°†æ•°æ®é€å…¥è®¾å¤‡
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        loss = outputs.loss.item()
    
    return loss


def score_candidate_word(
    text_with_mask: str,
    candidate_word: str,
    tokenizer,
    model,
    device: torch.device,
    max_length: int = 512,
    stride: int = 128
) -> float:
    """
    ä½¿ç”¨mask tokenè®¡ç®—å€™é€‰è¯çš„ç²¾ç¡®æŸå¤±ï¼ˆæ¨èæ–¹æ³•ï¼‰
    æ”¯æŒå•è¯å’Œå¤šè¯çŸ­è¯­ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£è‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬
    
    Args:
        text_with_mask: åŒ…å«mask tokençš„å¥å­
        candidate_word: å€™é€‰è¯æˆ–çŸ­è¯­
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        max_length: æœ€å¤§tokené•¿åº¦
        stride: æ»‘åŠ¨çª—å£æ­¥é•¿
        
    Returns:
        float: losså€¼ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    """
    # 1. å…ˆtokenizeå€™é€‰è¯ï¼Œçœ‹çœ‹å®ƒè¢«åˆ†æˆå¤šå°‘ä¸ªtoken
    candidate_tokens = tokenizer.tokenize(candidate_word)
    candidate_ids = tokenizer.convert_tokens_to_ids(candidate_tokens)
    
    # 2. æ ¹æ®å€™é€‰è¯çš„tokenæ•°é‡ï¼Œåˆ›å»ºå¯¹åº”æ•°é‡çš„mask
    if len(candidate_tokens) == 1:
        # å•ä¸ªtokençš„æƒ…å†µï¼Œä½¿ç”¨ç²¾ç¡®çš„maskæ–¹æ³•
        inputs = tokenizer(
            text_with_mask,
            return_tensors="pt",
            truncation=True,
            max_length=max_length,
            stride=stride,
            return_overflowing_tokens=True,
            padding=True
        )
        
        # æ‰¾åˆ°åŒ…å«mask tokençš„é‚£ä¸ªchunk
        mask_token_id = tokenizer.mask_token_id
        chunk_idx = 0
        
        # å¦‚æœæœ‰å¤šä¸ªchunksï¼Œæ‰¾åˆ°åŒ…å«maskçš„é‚£ä¸ª
        if "overflow_to_sample_mapping" in inputs:
            for idx in range(len(inputs["input_ids"])):
                if mask_token_id in inputs["input_ids"][idx]:
                    chunk_idx = idx
                    break
        
        # æ„é€ labelsï¼šåªåœ¨maskä½ç½®è®¡ç®—loss
        labels = torch.full_like(inputs["input_ids"], fill_value=-100)
        mask_positions = (inputs["input_ids"][chunk_idx] == mask_token_id).nonzero(as_tuple=True)[0]
        
        if len(mask_positions) > 0:
            labels[chunk_idx, mask_positions[0]] = candidate_ids[0]
        
        # åªä½¿ç”¨åŒ…å«maskçš„é‚£ä¸ªchunk
        selected_inputs = {
            "input_ids": inputs["input_ids"][chunk_idx:chunk_idx+1],
            "attention_mask": inputs["attention_mask"][chunk_idx:chunk_idx+1],
            "labels": labels[chunk_idx:chunk_idx+1]
        }
        
    else:
        # å¤šä¸ªtokençš„æƒ…å†µï¼Œç”¨å€™é€‰è¯æ›¿æ¢maskï¼Œè®¡ç®—æ•´ä½“loss
        text_with_candidate = text_with_mask.replace(tokenizer.mask_token, candidate_word)
        inputs = tokenizer(
            text_with_candidate,
            return_tensors="pt",
            truncation=True,
            max_length=max_length,
            stride=stride,
            return_overflowing_tokens=True,
            padding=True
        )
        
        # å¯¹äºå¤šè¯çŸ­è¯­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªchunkï¼ˆé€šå¸¸å€™é€‰è¯åœ¨å¼€å§‹éƒ¨åˆ†ï¼‰
        selected_inputs = {
            "input_ids": inputs["input_ids"][0:1],
            "attention_mask": inputs["attention_mask"][0:1],
            "labels": inputs["input_ids"][0:1].clone()
        }
    
    # å°†æ•°æ®é€å…¥è®¾å¤‡
    selected_inputs = {k: v.to(device) for k, v in selected_inputs.items()}
    
    # è®¡ç®—loss
    with torch.no_grad():
        outputs = model(**selected_inputs)
        return outputs.loss.item()


def clean_text(text: str, target_marker: str, option: str) -> str:
    """
    æ¸…ç†å’Œæ›¿æ¢æ–‡æœ¬ä¸­çš„æ ‡è®°
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        target_marker: ç›®æ ‡æ ‡è®°ï¼ˆå¦‚ "__1__"ï¼‰
        option: è¦å¡«å…¥çš„é€‰é¡¹
        
    Returns:
        str: æ¸…ç†åçš„æ–‡æœ¬
    """
    # æ›¿æ¢ç›®æ ‡æ ‡è®°
    current_test_text = text.replace(target_marker, option)
    # å°†å…¶ä»–é¢˜å·æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦
    clean_test_text = re.sub(r"__\d+__", "___", current_test_text)
    return clean_test_text


def crop_context(
    text: str,
    marker: str,
    context_size: int = 800
) -> str:
    """
    è£å‰ªæ–‡æœ¬ï¼Œä¿ç•™ç›®æ ‡æ ‡è®°å‘¨å›´çš„ä¸Šä¸‹æ–‡
    
    Args:
        text: å®Œæ•´æ–‡æœ¬
        marker: ç›®æ ‡æ ‡è®°
        context_size: ä¸Šä¸‹æ–‡å¤§å°ï¼ˆå‰åå„å–çš„å­—ç¬¦æ•°ï¼‰
        
    Returns:
        str: è£å‰ªåçš„æ–‡æœ¬
    """
    marker_pos = text.find(marker)
    if marker_pos == -1:
        return text
    
    start_pos = max(0, marker_pos - context_size)
    end_pos = min(len(text), marker_pos + context_size)
    return text[start_pos:end_pos]


def autoregressive_cloze_test(
    raw_text: str,
    options_dict: Dict[int, List[str]],
    tokenizer,
    model,
    device: torch.device,
    start_idx: int = 1,
    end_idx: int = 20,
    context_size: int = 800,  # ä¿ç•™å‚æ•°ä»¥å‘åå…¼å®¹ï¼Œä½†ä¸å†ä½¿ç”¨
    max_length: int = 512,
    letters: Optional[List[str]] = None
) -> Dict[int, Dict]:
    """
    è‡ªå›å½’å®Œå½¢å¡«ç©ºç­”é¢˜
    ä½¿ç”¨tokenizerçš„æ»‘åŠ¨çª—å£è‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬
    
    Args:
        raw_text: åŸå§‹æ–‡æœ¬ï¼ˆåŒ…å« __n__ æ ‡è®°ï¼‰
        options_dict: é€‰é¡¹å­—å…¸ {é¢˜å·: [é€‰é¡¹åˆ—è¡¨]}
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        start_idx: èµ·å§‹é¢˜å·
        end_idx: ç»“æŸé¢˜å·ï¼ˆä¸åŒ…å«ï¼‰
        context_size: ï¼ˆå·²å¼ƒç”¨ï¼‰ä¿ç•™ä»¥å‘åå…¼å®¹ï¼Œtokenizerè‡ªåŠ¨å¤„ç†æ»‘åŠ¨çª—å£
        max_length: æœ€å¤§tokené•¿åº¦
        letters: é€‰é¡¹å­—æ¯åˆ—è¡¨ï¼Œé»˜è®¤ä¸º ["A", "B", "C", "D"]
        
    Returns:
        dict: {é¢˜å·: {"option": é€‰é¡¹, "letter": å­—æ¯, "loss": losså€¼}}
    """
    if letters is None:
        letters = ["A", "B", "C", "D"]
    
    print("\n--- å¼€å§‹å¸¦ GPU åŠ é€Ÿçš„è‡ªå›å½’ç­”é¢˜ ---")
    
    working_text = raw_text
    results_summary = {}
    
    for i in range(start_idx, end_idx + 1):
        if i not in options_dict:
            continue
            
        target_marker = f"__{i}__"
        results = {}
        
        # å°†ç›®æ ‡æ ‡è®°æ›¿æ¢ä¸º mask token
        text_with_mask = working_text.replace(target_marker, tokenizer.mask_token)
        # å°†å…¶ä»–é¢˜å·æ ‡è®°æ›¿æ¢ä¸ºå ä½ç¬¦
        clean_text_with_mask = re.sub(r"__\d+__", "___", text_with_mask)
        
        # ä¸å†éœ€è¦æ‰‹åŠ¨è£å‰ªï¼Œtokenizerä¼šè‡ªåŠ¨ç”¨æ»‘åŠ¨çª—å£å¤„ç†é•¿æ–‡æœ¬
        for opt in options_dict[i]:
            # ä½¿ç”¨æ–°çš„åŸºäºmaskçš„è¯„åˆ†æ–¹æ³•ï¼ˆè‡ªå¸¦æ»‘åŠ¨çª—å£ï¼‰
            loss = score_candidate_word(
                clean_text_with_mask,
                opt,
                tokenizer,
                model,
                device,
                max_length
            )
            
            results[opt] = loss
        
        # æ‰¾å‡º Loss æœ€ä½çš„é€‰é¡¹
        best_opt = min(results, key=results.get)
        best_loss = results[best_opt]
        best_idx = options_dict[i].index(best_opt)
        best_letter = letters[best_idx]
        
        # å¡«å…¥ç­”æ¡ˆï¼Œå®ç°è‡ªå›å½’
        working_text = working_text.replace(target_marker, best_opt)
        
        # ä¿å­˜ç»“æœ
        results_summary[i] = {
            "option": best_opt,
            "letter": best_letter,
            "loss": best_loss,
            "all_scores": results
        }
        
        print(
            f"ç¬¬ {i:02d} é¢˜ -> æ¨¡å‹é€‰æ‹©: {best_letter}. {best_opt} (Loss: {best_loss:.4f})"
        )
    
    print("\n--- ç­”é¢˜ç»“æŸ ---")
    return results_summary


def simple_cloze_test(
    prompt_template: str,
    options: List[str],
    tokenizer,
    model,
    device: torch.device
) -> Dict[str, float]:
    """
    ç®€å•çš„å®Œå½¢å¡«ç©ºæµ‹è¯•ï¼ˆå•é¢˜ï¼‰
    
    Args:
        prompt_template: æç¤ºæ¨¡æ¿ï¼Œä½¿ç”¨ {} ä½œä¸ºå ä½ç¬¦
        options: é€‰é¡¹åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        model: MLMæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        dict: {é€‰é¡¹: losså€¼}ï¼ŒæŒ‰lossä»å°åˆ°å¤§æ’åº
    """
    print("\n--- å¼€å§‹å…è®­ç»ƒ(Zero-shot)æ‰“åˆ† ---")
    
    results = {}
    
    for opt in options:
        complete_sentence = prompt_template.format(opt)
        loss_score = score_sentence(complete_sentence, tokenizer, model, device)
        results[opt] = loss_score
    
    sorted_results = sorted(results.items(), key=lambda x: x[1])
    
    for rank, (word, loss) in enumerate(sorted_results, 1):
        if rank == 1:
            print(f"ğŸ† æœ€ä½³é€‰é¡¹ -> {word} (ä¸è‡ªç„¶åº¦ Loss: {loss:.4f})")
        else:
            print(f"   æ·˜æ±°é€‰é¡¹ -> {word} (ä¸è‡ªç„¶åº¦ Loss: {loss:.4f})")
    
    return dict(sorted_results)


def print_results_table(
    results: Dict[int, Dict],
    answers: Optional[Dict[int, str]] = None
) -> None:
    """
    æ‰“å°ç­”é¢˜ç»“æœè¡¨æ ¼
    
    Args:
        results: autoregressive_cloze_test è¿”å›çš„ç»“æœ
        answers: æ ‡å‡†ç­”æ¡ˆå­—å…¸ {é¢˜å·: "å­—æ¯"}
    """
    print("\n| é¢˜å· | æ¨¡å‹é€‰æ‹© | æ ‡å‡†ç­”æ¡ˆ | æ‰¹æ”¹ |")
    print("|---:|:---|:---|:---:|")
    
    for i in sorted(results.keys()):
        model_choice = f"{results[i]['letter']}. {results[i]['option']}"
        
        if answers and i in answers:
            is_correct = results[i]['letter'] == answers[i]
            mark = "âœ…" if is_correct else "âŒ"
            print(f"| {i:02d} | {model_choice} | {answers[i]} | {mark} |")
        else:
            print(f"| {i:02d} | {model_choice} | - | - |")
